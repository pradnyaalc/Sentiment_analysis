{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "import logging\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import word2vec\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\palc0001\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install gensim\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data = pd.read_csv('./Training Dataset-20191023/labeled_data.csv')\n",
    "unlabeled_data = pd.read_csv('./Training Dataset-20191023/unlabeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>the new rule is if you are waiting for a table...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>flirted with giving this two star but that's a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>i wa staying at planet hollywood across the st...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>food is good but price are super expensive buc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>worse company to deal with they do horrible wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  the new rule is if you are waiting for a table...      4\n",
       "1  flirted with giving this two star but that's a...      3\n",
       "2  i wa staying at planet hollywood across the st...      5\n",
       "3  food is good but price are super expensive buc...      2\n",
       "4  worse company to deal with they do horrible wo...      1"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flirted with giving this two stars, but that\\'s a pretty damning rating for what might have just been an off night...\\r\\n\\r\\nNew to the East side, and so we don\\'t know many of these hidden gems, but me and the fiance met her friend for drinks here and ended up getting some things to nibble. \\r\\n\\r\\nFirst off, service was pretty slow, which was unusual because the restaurant is pretty small and galley style. You would think it would be easy for servers to routinely hit up tables as you pass by. \\r\\n\\r\\nThe fiance ordered the Quinoa Salad, and said it was pretty good, but dry. I wasn\\'t too hungry and so I simply ordered the Bruchetta 3-way which came with burnt crostinis. And I ordered a side of fries, which were either hard or chewy.\\r\\n\\r\\nThe friend ordered the macaroni & cheese, and added chicken and bacon (her usual order) and liked it.  \\r\\n\\r\\nCan\\'t remember the last time I thought to myself- \"Huh... they failed at fries...\" So, like I said- two stars. But, the decor was good, it was a good place to have a conversation, and I might be back to try more expensive fare, but-... ah... the fry thing... yeeesh... I dunno, man...'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_data['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_characters(raw_text):\n",
    "    processed_text = re.sub('\\\\n','', raw_text)\n",
    "    processed_text = re.sub('\\\\r','', processed_text)\n",
    "    processed_text = re.sub(\"\\\\'\", \"\\'\",processed_text)\n",
    "    processed_text = re.sub(r'\\d+','', processed_text)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data['text'] = lab_data.apply(lambda row: remove_extra_characters(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Normalisation, Tokenization and Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(token_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_token = []\n",
    "    for each in token_list :\n",
    "#         print(each ,\":\", lemmatizer.lemmatize(each)) \n",
    "        lem_token.append(lemmatizer.lemmatize(each))\n",
    "    return lem_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"\\w+(?:[']\\w+)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(raw_data):\n",
    "    raw_data1 = raw_data.lower()\n",
    "    tokenised = tokenizer.tokenize(raw_data1)\n",
    "#     tokenised = nltk.tokenize.word_tokenize(raw_data1)\n",
    "    lem_token = lemmatization(tokenised)\n",
    "#     stopwords_tokens = [w for w in tokenised if not w in stopwords]\n",
    "    processed_data = ' '.join(lem_token)\n",
    "        \n",
    "    return(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lab_data['text'] = lab_data.apply(lambda row: token(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"flirted with giving this two star but that's a pretty damning rating for what might have just been an off night new to the east side and so we don't know many of these hidden gem but me and the fiance met her friend for drink here and ended up getting some thing to nibble first off service wa pretty slow which wa unusual because the restaurant is pretty small and galley style you would think it would be easy for server to routinely hit up table a you pas by the fiance ordered the quinoa salad and said it wa pretty good but dry i wasn't too hungry and so i simply ordered the bruchetta way which came with burnt crostinis and i ordered a side of fry which were either hard or chewy the friend ordered the macaroni cheese and added chicken and bacon her usual order and liked it can't remember the last time i thought to myself huh they failed at fry so like i said two star but the decor wa good it wa a good place to have a conversation and i might be back to try more expensive fare but ah the fry thing yeeesh i dunno man\""
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_data['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Had a good experience when my wife and I sat at the bar. Great pizza and wings. \\r\\n\\r\\nHowever, we tried to go recently with a larger group (8 people) and it was 1.25 hr wait. At 5pm on a Wednesday... Riiiiight.  \\r\\n\\r\\nI tried to call ahead and they don't accept call aheads. They apparently only have 1 table capable of seating larger parties. Kinda missed the mark on that one Oreganos. Brand spankin new building and all. \\r\\n\\r\\nSo we went across the street to Native NewYorker and got seated immediately.\""
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data['text'] = unlabeled_data.apply(lambda row: remove_extra_characters(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data['text'] = unlabeled_data.apply(lambda row: token(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"had a good experience when my wife and i sat at the bar great pizza and wing however we tried to go recently with a larger group people and it wa hr wait at pm on a wednesday riiiiight i tried to call ahead and they don't accept call aheads they apparently only have table capable of seating larger party kinda missed the mark on that one oregano brand spankin new building and all so we went across the street to native newyorker and got seated immediately\""
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_data['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase = True,analyzer = 'word',ngram_range = (1,2), min_df=3, max_df=.99)\n",
    "    \n",
    "train_review = vectorizer.fit_transform(lab_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_review, lab_data['label'],test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_cross_val(model):\n",
    "    # perfroming 10 fold cross validation\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    params = {}\n",
    "    nb = model\n",
    "    gs = GridSearchCV(nb, cv=skf, param_grid=params, return_train_score=False)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "gs = instantiate_cross_val(model)\n",
    "\n",
    "clf=gs.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('parameters:', clf.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(random_state=1, C=1, solver='sag', multi_class = 'multinomial')\n",
    "log_model.fit(X_train, y_train)\n",
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6123\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# multi_class = ['multinomial','ovr']\n",
    "\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(model, hyperparameters, cv=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best C:', best_model.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_best_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_best_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.719626168224299,\n",
       "  'recall': 0.765788165091994,\n",
       "  'f1-score': 0.741989881956155,\n",
       "  'support': 2011},\n",
       " '2': {'precision': 0.5308947108255067,\n",
       "  'recall': 0.5327380952380952,\n",
       "  'f1-score': 0.5318148056449616,\n",
       "  'support': 2016},\n",
       " '3': {'precision': 0.5209549071618037,\n",
       "  'recall': 0.49570923775870773,\n",
       "  'f1-score': 0.5080186239006725,\n",
       "  'support': 1981},\n",
       " '4': {'precision': 0.5334665334665335,\n",
       "  'recall': 0.5415821501014199,\n",
       "  'f1-score': 0.5374937091092099,\n",
       "  'support': 1972},\n",
       " '5': {'precision': 0.7348717948717949,\n",
       "  'recall': 0.7094059405940594,\n",
       "  'f1-score': 0.7219143576826196,\n",
       "  'support': 2020},\n",
       " 'accuracy': 0.6097,\n",
       " 'macro avg': {'precision': 0.6079628229099875,\n",
       "  'recall': 0.6090447177568553,\n",
       "  'f1-score': 0.6082462756587237,\n",
       "  'support': 10000},\n",
       " 'weighted avg': {'precision': 0.608590066204785,\n",
       "  'recall': 0.6097,\n",
       "  'f1-score': 0.6088869791623556,\n",
       "  'support': 10000}}"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "classifier_linear.fit(X_train, y_train)\n",
    "prediction_linear = classifier_linear.predict(X_test)\n",
    "# results\n",
    "classification_report(y_test, prediction_linear, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6097\n"
     ]
    }
   ],
   "source": [
    "print(metrics.accuracy_score( y_test,prediction_linear))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X_data, y_data, batch_size):\n",
    "    dim = X_data.shape[1]\n",
    "    samples_per_epoch = X_data.shape[0]\n",
    "    number_of_batches = samples_per_epoch/batch_size\n",
    "    counter=0\n",
    "    index = np.arange(np.shape(y_data)[0])\n",
    "    while 1:\n",
    "        index_batch = index[batch_size*counter:batch_size*(counter+1)]\n",
    "        X_batch = X_data[index_batch,:].toarray()\n",
    "        y_batch = y_data[y_data.index[index_batch]]\n",
    "        counter += 1\n",
    "        yield X_batch,y_batch\n",
    "        if (counter > number_of_batches):\n",
    "            counter=0\n",
    "            \n",
    "model = Sequential()\n",
    "model.add(Dense(64, activation='relu', input_dim=dim))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit_generator(generator=batch_generator(X_train, y_train, 32),\n",
    "                    epochs=5, validation_data=(X_test, y_test),\n",
    "                    steps_per_epoch=X_train.shape[0]/32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=['1', '2', '3', '4', '5'],\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict unlabeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"had a good experience when my wife and i sat at the bar great pizza and wing however we tried to go recently with a larger group people and it wa hr wait at pm on a wednesday riiiiight i tried to call ahead and they don't accept call aheads they apparently only have table capable of seating larger party kinda missed the mark on that one oregano brand spankin new building and all so we went across the street to native newyorker and got seated immediately\""
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_data['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_test = vectorizer.transform(unlabeled_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_class = log_model.predict(unlabeled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_class[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probab = log_model.predict_proba(unlabeled_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23060546, 0.31986946, 0.19747045, 0.13878374, 0.11327089])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_probab[10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred_probab)):\n",
    "    p_test.append(max(pred_probab[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_data['label'] = pred_class\n",
    "unlabeled_data['probability'] = p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>had a good experience when my wife and i sat a...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.404494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>on my first to montreal with my gf we came her...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.402237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>one of our favorite place to go when it's cold...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.711828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>the doctor wa very nice got in in a good amoun...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.605171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>the nook is an immediate phoenix staple i came...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.771637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label  probability\n",
       "0  had a good experience when my wife and i sat a...      3     0.404494\n",
       "1  on my first to montreal with my gf we came her...      4     0.402237\n",
       "2  one of our favorite place to go when it's cold...      5     0.711828\n",
       "3  the doctor wa very nice got in in a good amoun...      1     0.605171\n",
       "4  the nook is an immediate phoenix staple i came...      5     0.771637"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_data = unlabeled_data[unlabeled_data['probability'] > 0.8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>a hidden gem great cake love this place good s...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.826098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>the worst system ever the black box sock only ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.802013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>not pleased with customer service we patiently...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.856839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>i don't write review often i only do it when i...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.898055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>do not get your policy with them worst custome...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.963723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599962</td>\n",
       "      <td>bought a groupon and had a great time my wife ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.878433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599968</td>\n",
       "      <td>where else will a business answer their phone ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.844817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599970</td>\n",
       "      <td>worse dme company ever day before my due date ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.923103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599984</td>\n",
       "      <td>worst mcdonalds i've ever been to on multiple ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.926845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599986</td>\n",
       "      <td>bought a wig here i absolutely loved it great ...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.813545</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56436 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label  probability\n",
       "15      a hidden gem great cake love this place good s...      5     0.826098\n",
       "22      the worst system ever the black box sock only ...      1     0.802013\n",
       "27      not pleased with customer service we patiently...      1     0.856839\n",
       "38      i don't write review often i only do it when i...      1     0.898055\n",
       "88      do not get your policy with them worst custome...      1     0.963723\n",
       "...                                                   ...    ...          ...\n",
       "599962  bought a groupon and had a great time my wife ...      5     0.878433\n",
       "599968  where else will a business answer their phone ...      1     0.844817\n",
       "599970  worse dme company ever day before my due date ...      1     0.923103\n",
       "599984  worst mcdonalds i've ever been to on multiple ...      1     0.926845\n",
       "599986  bought a wig here i absolutely loved it great ...      5     0.813545\n",
       "\n",
       "[56436 rows x 3 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palc0001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:4102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>a hidden gem great cake love this place good s...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>the worst system ever the black box sock only ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>not pleased with customer service we patiently...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>i don't write review often i only do it when i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>do not get your policy with them worst custome...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599962</td>\n",
       "      <td>bought a groupon and had a great time my wife ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599968</td>\n",
       "      <td>where else will a business answer their phone ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599970</td>\n",
       "      <td>worse dme company ever day before my due date ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599984</td>\n",
       "      <td>worst mcdonalds i've ever been to on multiple ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599986</td>\n",
       "      <td>bought a wig here i absolutely loved it great ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>56436 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "15      a hidden gem great cake love this place good s...      5\n",
       "22      the worst system ever the black box sock only ...      1\n",
       "27      not pleased with customer service we patiently...      1\n",
       "38      i don't write review often i only do it when i...      1\n",
       "88      do not get your policy with them worst custome...      1\n",
       "...                                                   ...    ...\n",
       "599962  bought a groupon and had a great time my wife ...      5\n",
       "599968  where else will a business answer their phone ...      1\n",
       "599970  worse dme company ever day before my due date ...      1\n",
       "599984  worst mcdonalds i've ever been to on multiple ...      1\n",
       "599986  bought a wig here i absolutely loved it great ...      5\n",
       "\n",
       "[56436 rows x 2 columns]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_train_data.drop(['probability'], axis=1, inplace=True)\n",
    "new_train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106436"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.concat([lab_data, new_train_data])\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>the new rule is if you are waiting for a table...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>flirted with giving this two star but that's a...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>i wa staying at planet hollywood across the st...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>food is good but price are super expensive buc...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>worse company to deal with they do horrible wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599962</td>\n",
       "      <td>bought a groupon and had a great time my wife ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599968</td>\n",
       "      <td>where else will a business answer their phone ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599970</td>\n",
       "      <td>worse dme company ever day before my due date ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599984</td>\n",
       "      <td>worst mcdonalds i've ever been to on multiple ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>599986</td>\n",
       "      <td>bought a wig here i absolutely loved it great ...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>106436 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0       the new rule is if you are waiting for a table...      4\n",
       "1       flirted with giving this two star but that's a...      3\n",
       "2       i wa staying at planet hollywood across the st...      5\n",
       "3       food is good but price are super expensive buc...      2\n",
       "4       worse company to deal with they do horrible wo...      1\n",
       "...                                                   ...    ...\n",
       "599962  bought a groupon and had a great time my wife ...      5\n",
       "599968  where else will a business answer their phone ...      1\n",
       "599970  worse dme company ever day before my due date ...      1\n",
       "599984  worst mcdonalds i've ever been to on multiple ...      1\n",
       "599986  bought a wig here i absolutely loved it great ...      5\n",
       "\n",
       "[106436 rows x 2 columns]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "\n",
    "vectorizer_new = TfidfVectorizer(lowercase = True,analyzer = 'word',ngram_range = (1,2), min_df=3, max_df=.99)\n",
    "    \n",
    "train = vectorizer_new.fit_transform(train_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train, train_data['label'],test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model_new = LogisticRegression(random_state=1, C=1, solver='sag', multi_class = 'multinomial')\n",
    "log_model_new.fit(X_train, y_train)\n",
    "y_pred = log_model_new.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8129932356257046\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec + Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_data_1 = pd.DataFrame({'review':unlabeled_data['text']})\n",
    "train_data_2 = pd.DataFrame({'review':lab_data['text']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "650000"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.concat([train_data_1, train_data_2])\n",
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>had a good experience when my wife and i sat a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>on my first to montreal with my gf we came her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>one of our favorite place to go when it's cold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>the doctor wa very nice got in in a good amoun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>the nook is an immediate phoenix staple i came...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review\n",
       "0  had a good experience when my wife and i sat a...\n",
       "1  on my first to montreal with my gf we came her...\n",
       "2  one of our favorite place to go when it's cold...\n",
       "3  the doctor wa very nice got in in a good amoun...\n",
       "4  the nook is an immediate phoenix staple i came..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentences = []\n",
    "# for review in lab_data['text']:\n",
    "#     sentences.append(review.split())\n",
    "# for review in unlabeled_data['text']:\n",
    "#     sentences.append(review.split)\n",
    "sentences = train_data.apply(lambda row: row['review'].split(), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['had', 'a', 'good', 'experience', 'when', 'my', 'wife', 'and', 'i', 'sat', 'at', 'the', 'bar', 'great', 'pizza', 'and', 'wing', 'however', 'we', 'tried', 'to', 'go', 'recently', 'with', 'a', 'larger', 'group', '8', 'people', 'and', 'it', 'wa', '1', '25', 'hr', 'wait', 'at', '5pm', 'on', 'a', 'wednesday', 'riiiiight', 'i', 'tried', 'to', 'call', 'ahead', 'and', 'they', \"don't\", 'accept', 'call', 'aheads', 'they', 'apparently', 'only', 'have', '1', 'table', 'capable', 'of', 'seating', 'larger', 'party', 'kinda', 'missed', 'the', 'mark', 'on', 'that', 'one', 'oregano', 'brand', 'spankin', 'new', 'building', 'and', 'all', 'so', 'we', 'went', 'across', 'the', 'street', 'to', 'native', 'newyorker', 'and', 'got', 'seated', 'immediately']),\n",
       "       list(['on', 'my', 'first', 'to', 'montreal', 'with', 'my', 'gf', 'we', 'came', 'here', 'to', 'eat', 'a', 'nice', 'mid', 'day', 'lunch', 'before', 'walking', 'around', 'more', 'and', 'departing', 'from', 'montreal', 'now', 'obviously', 'i', 'am', 'one', 'that', 'ha', 'a', 'love', 'hate', 'relationship', 'with', 'spot', 'that', 'are', 'touristy', 'and', 'well', 'known', 'but', 'in', 'this', 'case', 'it', 'wa', 'worth', 'it', 'i', 'liked', 'the', 'fact', 'that', 'we', 'walked', 'down', 'a', 'nice', 'cobblestone', 'corridor', 'then', 'wa', 'greeted', 'by', 'the', 'hostess', 'who', 'promptly', 'showed', 'u', 'to', 'our', 'table', 'our', 'seating', 'wa', 'right', 'in', 'front', 'where', 'they', 'had', 'two', 'jazz', 'player', 'performing', 'throughout', 'the', 'lunch', 'playing', 'soft', 'melody', 'for', 'all', 'of', 'the', 'diner', 'entertainment', 'and', 'i', 'always', 'believe', 'with', 'soothing', 'music', 'a', 'meal', 'is', 'always', 'made', 'more', 'enjoyable', 'looking', 'around', 'this', 'wa', 'definite', 'a', 'place', 'i', 'could', 'fall', 'in', 'love', 'with', 'beautiful', 'flower', 'bed', 'arrangement', 'with', 'different', 'color', 'that', 'made', 'the', 'restaurant', 'really', 'pop', 'the', 'large', \"umbrella's\", 'which', 'provided', 'a', 'nice', 'cool', 'atmosphere', 'and', 'most', 'importantly', 'shade', 'plus', 'bird', 'flying', 'around', 'during', 'the', 'meal', 'and', 'nicely', 'arranged', 'plant', 'i', 'loved', 'the', 'ambiance', 'the', 'food', 'on', 'the', 'other', 'hand', 'could', 'be', 'improved', 'with', 'such', 'a', 'nicely', 'designed', 'restaurant', 'i', 'would', 'expect', 'a', 'vast', 'selection', 'also', 'my', 'gf', 'suggested', 'that', 'when', 'your', 'in', 'a', 'new', 'place', 'you', 'should', 'try', 'new', 'food', 'and', 'i', '100', 'agree', 'with', 'that', 'but', 'in', 'this', 'case', 'i', 'should', 'have', 'just', 'gotten', 'the', 'salmon', 'because', 'everything', 'else', 'i', 'looked', 'at', 'on', 'the', 'menu', 'i', 'just', 'wa', 'not', 'feeling', 'had', 'the', 'veal', 'meatball', 'style', 'dish', 'and', 'it', \"wasn't\", 'that', 'great', \"i'd\", 'say', 'it', 'wa', 'at', 'best', 'a', '6', 'out', 'of', '10', 'if', 'i', 'had', 'to', 'grade', 'it', 'but', 'overall', 'the', 'decor', 'ambiance', 'made', 'up', 'for', 'that', 'plus', 'our', 'waitress', 'wa', 'extremely', 'helpful', 'friendly', 'i', 'also', 'agree', 'with', 'some', 'of', 'the', 'review', 'a', 'bit', 'overpriced', 'for', 'what', 'they', 'were', 'serving']),\n",
       "       list(['one', 'of', 'our', 'favorite', 'place', 'to', 'go', 'when', \"it's\", 'cold', 'and', 'rainy', 'out', 're', 'defines', 'comfort', 'food', 'and', 'you', 'have', 'to', 'try', 'the', 'rice', 'crispy', 'square', 'the', 'best']),\n",
       "       ...,\n",
       "       list(['poor', 'customer', 'service', 'wa', 'just', 'going', 'to', 'ask', 'if', 'guest', 'staying', 'at', 'the', 'hotel', 'or', 'basic', 'membership', 'holder', 'can', 'use', 'the', 'fast', 'lane', 'you', \"won't\", 'believe', 'what', 'the', 'staff', 'at', 'the', 'entrance', 'said', 'after', 'telling', 'u', 'no', 'he', 'actually', 'point', 'at', 'the', 'sign', 'there', 'with', 'a', 'attitude', 'that', 'meaning', \"can't\", 'you', 'read', 'why', 'would', 'i', 'ask', 'you', 'if', 'the', 'sign', 'is', 'clear', 'enough', 'i', 'feel', 'so', 'offended', 'and', 'just', 'walk', 'away', 'and', 'went', 'to', 'cesar', 'instead', 'my', 'check', 'in', 'experience', 'with', 'front', 'desk', 'is', 'unpleasant', 'a', 'well', 'he', \"didn't\", 'even', 'border', 'to', 'take', 'the', 'time', 'to', 'greet', 'you', 'or', 'make', 'you', 'feel', 'welcomed', 'high', 'end', 'hotel', 'with', 'terrible', 'customer', 'service', 'first', 'and', 'last', 'time']),\n",
       "       list(['when', 'i', 'need', 'some', 'rover', 'part', 'in', 'a', 'hurry', 'i', 'have', 'to', 'come', 'here', 'to', 'purchase', 'instead', 'of', 'online', 'these', 'guy', 'have', 'always', 'taken', 'care', 'of', 'me', 'and', 'give', 'me', 'the', 'brother', 'in', 'law', 'price', 'on', 'part', 'i', \"don't\", 'know', 'what', 'it', 'mean', 'but', 'it', 'always', 'work', 'when', 'you', 'go', 'to', 'a', 'place', 'that', 'sale', 'both', 'wholesale', 'retail', 'ask', 'them', 'if', 'thats', 'the', 'bro', 'in', 'law', 'price', 'and', 'they', 'usually', 'drop', 'it', 'a', 'little', 'more', 'the', 'part', 'guy', 'are', 'knowledgeable', 'and', 'will', 'give', 'you', 'advice', 'where', 'to', 'pick', 'up', 'stuff', 'cheaper', 'great', 'guy']),\n",
       "       list(['went', 'in', 'to', 'buy', '1', 'item', 'and', 'spent', '20', 'min', 'in', 'line', 'keep', 'in', 'mind', 'there', 'wa', 'only', '1', 'person', 'in', 'line', 'before', 'me', 'i', 'waited', 'thinking', 'the', 'person', 'with', 'the', 'cashier', 'may', 'have', 'a', 'big', 'order', 'no', 'problem', 'but', 'while', 'she', 'wa', 'with', 'them', '3', 'other', 'employee', 'were', 'filling', 'balloon', 'for', 'an', 'order', 'the', 'line', 'grew', 'to', '8', '12', 'people', 'and', 'i', 'wa', 'still', 'second', 'in', 'line', 'wondering', 'will', 'one', 'of', 'the', 'others', 'jump', 'on', 'a', 'register', 'and', 'help', 'move', 'the', 'line', 'nope', 'they', 'just', 'kept', 'filling', 'the', 'balloon', 'i', 'thought', 'customer', 'service', 'wa', 'to', 'handle', 'every', 'client', 'not', 'just', '1', 'oh', 'well', 'i', 'left', 'without', 'the', 'item', 'i', 'came', 'in', 'for'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import phrases\n",
    "bigrams = phrases.Phrases(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'new_york']\n"
     ]
    }
   ],
   "source": [
    "print(bigrams[\"this is the new york\".split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 5   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "# context = 10          # Context window size                                                                                    \n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(bigrams[sentences], workers=num_workers, \\\n",
    "            size=num_features, min_count=3)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "# model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?word2vec.Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palc0001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\palc0001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py:877: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
      "  vectors = vstack(self.word_vec(word, use_norm=True) for word in used_words).astype(REAL)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palc0001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(109094, 300)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "list(islice(model.wv.vocab, 11030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(lab_data, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = text.split(' ')\n",
    "    return tokens\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['i', 'found', 'this', 'place', 'to', 'be', 'overpriced', 'especially', 'the', 'food', 'the', 'appetizer', 'we', 'got', 'were', 'not', 'worth', 'the', 'money', 'especially', 'when', 'it', 'came', 'to', 'the', 'portion', 'if', 'i', 'do', 'come', 'back', \"i'll\", 'make', 'sure', 'to', 'eat', 'before', 'i', 'go', 'but', 'i', 'think', 'this', 'would', 'be', 'a', 'good', 'place', 'to', 'go', 'when', 'the', 'weather', 'is', 'nice', 'since', 'the', 'back', 'patio', 'look', 'relaxing', 'if', 'it', \"weren't\", 'for', 'that', \"i'm\", 'not', 'sure', 'i', 'would', 'return']),\n",
       "       list(['delicious', 'food', 'good', 'coffee', 'very', 'friendly', 'food', 'portion', 'were', 'average', 'for', 'la', 'vega', 'my', 'youngest', 'had', 'pancake', 'with', 'egg', 'and', 'bacon', 'pancake', 'were', 'large', 'fluffy', 'and', 'moist', 'hubby', 'had', 'corn', 'beef', 'hash', 'with', 'over', 'easy', 'egg', 'and', 'it', 'wa', 'hash', 'a', 'i', 'have', 'never', 'seen', 'it', 'like', 'cut', 'up', 'slice', 'of', 'corn', 'beef', 'tasted', 'pretty', 'good', 'and', 'not', 'greasy', 'at', 'all', 'older', 'daughter', 'had', 'egg', 'burrito', 'with', 'chorizo', 'she', 'loved', 'every', 'bit', 'and', 'opted', 'for', 'fresh', 'fruit', 'in', 'place', 'of', 'potato', 'piece', 'of', 'watermelon', 'cantaloupe', 'and', 'grapefruit', 'i', 'had', 'scrambled', 'egg', 'with', 'green', 'onion', 'and', 'cream', 'cheese', 'side', 'of', 'hash', 'brown', 'and', 'in', 'place', 'of', 'toast', 'coffee', 'cake', 'coffee', 'cake', 'wa', 'amazing', 'super', 'moist', 'and', 'perfect', 'amount', 'of', 'cinnamon', 'delicious', 'my', 'egg', 'with', 'cream', 'cheese', 'and', 'green', 'onion', 'were', 'flavorful', 'and', 'moist', 'coffee', 'hot', 'and', 'strong', 'service', 'fast', 'friendly', 'and', 'server', 'stopped', 'by', 'many', 'time', 'to', 'refill', 'beverage', 'bathroom', 'cramped', 'but', 'very', 'clean', 'only', '4', 'star', 'because', 'i', 'feel', 'the', 'price', 'wa', 'a', 'bit', 'high', 'over', '50', '00', 'for', 'our', 'meal', 'i', \"don't\", 'know', 'if', 'that', 'is', 'average', 'for', 'off', 'the', 'strip', 'but', 'for', 'u', 'so', 'cal', 'family', \"it's\", 'a', 'bit', 'high', 'with', 'all', 'that', 'said', 'will', 'go', 'back', 'again']),\n",
       "       list(['i', 'got', 'an', 'odd', 'feeling', 'that', 'i', 'wa', 'in', 'a', 'flea', 'market', 'of', 'crazy', 'expensive', 'watch', 'must', 'have', 'been', 'the', 'salesperson', 'either', 'pushy', 'or', 'distant', \"i'll\", 'stick', 'with', 'our', 'baby', 'tourneau', 'in', 'scottsdale']),\n",
       "       ...,\n",
       "       list(['great', 'staff', 'great', 'vet', 'had', 'a', 'great', 'experience', 'with', 'my', 'new', 'puppy', 'they', 'also', 'have', 'really', 'good', 'price', 'with', 'different', 'package']),\n",
       "       list(['have', 'loved', 'this', 'restaurant', 'since', 'the', 'first', 'time', 'i', 'ever', 'ate', 'here', 'i', 'own', 'a', 'salon', 'in', 'the', 'area', 'and', 'can', 'say', 'that', 'il', 'bosco', 'is', 'responsible', 'for', 'feeding', 'u', 'multiple', 'day', 'out', 'of', 'the', 'week', 'love', 'the', 'changing', 'special', 'and', 'charm', 'of', 'the', 'quaint', 'atmosphere', 'staff', 'is', 'friendly', 'and', 'accommodating', 'but', 'not', 'overly', 'intrusive', 'love', 'the', 'caprese', 'panini', 'and', 'the', 'biaggo', 'pizza', 'all', 'the', 'food', 'is', 'extremely', 'fresh', 'and', 'always', 'tasty']),\n",
       "       list(['my', 'nail', 'tech', 'candy', 'deserves', '5', 'star', 'she', 'is', 'friendly', 'and', 'doe', 'a', 'great', 'job', 'the', 'salon', 'is', 'also', 'pretty', 'and', 'nicely', 'decorated', 'however', 'they', 'use', 'horrible', 'polish', 'the', 'selection', 'of', 'color', 'is', 'minimal', 'i', 'paid', 'extra', 'for', 'gel', 'and', 'it', 'peeled', 'the', 'next', 'day', 'my', 'toe', 'had', 'regular', 'polish', 'and', 'they', \"didn't\", 'last', '5', 'day', 'i', 'had', 'a', 'package', 'of', 'groupons', 'and', 'each', 'time', 'my', 'polish', 'lasted', '1', '5', 'day', 'i', 'recently', 'thought', \"i'd\", 'give', 'it', 'another', 'try', 'because', 'i', 'like', 'the', 'tech', 'so', 'much', 'but', 'they', \"won't\", 'respond', 'to', 'my', 'message', 'i', 'asked', 'if', 'she', 'worked', 'there', 'and', 'if', \"they've\", 'changed', 'the', 'polish', 'they', 'use', 'and', 'they', \"won't\", 'text', 'back', 'pretty', 'disappointing'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_tokenized = test['text'].values\n",
    "train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\palc0001\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  \n",
      "WARNING:root:cannot compute similarity with no input ['']\n",
      "WARNING:root:cannot compute similarity with no input ['']\n"
     ]
    }
   ],
   "source": [
    "X_train_word_average = word_averaging_list(model.wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(model.wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5972666666666666\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(random_state=1, C=2, solver='sag', multi_class = 'multinomial')\n",
    "logreg.fit(X_train_word_average, train['label'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "print('accuracy %s' % metrics.accuracy_score(y_pred, test.label))\n",
    "# print(classification_report(test.label, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': {'precision': 0.7084106369820655,\n",
       "  'recall': 0.7551087673038892,\n",
       "  'f1-score': 0.731014677728143,\n",
       "  'support': 3034},\n",
       " '2': {'precision': 0.5207357859531773,\n",
       "  'recall': 0.519,\n",
       "  'f1-score': 0.5198664440734557,\n",
       "  'support': 3000},\n",
       " '3': {'precision': 0.48853132488873674,\n",
       "  'recall': 0.4932595921189077,\n",
       "  'f1-score': 0.4908840729274166,\n",
       "  'support': 2893},\n",
       " '4': {'precision': 0.5197498354180382,\n",
       "  'recall': 0.5295103957075789,\n",
       "  'f1-score': 0.5245847176079734,\n",
       "  'support': 2982},\n",
       " '5': {'precision': 0.7387291444799432,\n",
       "  'recall': 0.6732449045616306,\n",
       "  'f1-score': 0.7044685172647258,\n",
       "  'support': 3091},\n",
       " 'accuracy': 0.5956666666666667,\n",
       " 'macro avg': {'precision': 0.5952313455443923,\n",
       "  'recall': 0.5940247319384013,\n",
       "  'f1-score': 0.594163685920343,\n",
       "  'support': 15000},\n",
       " 'weighted avg': {'precision': 0.5972101432113552,\n",
       "  'recall': 0.5956666666666667,\n",
       "  'f1-score': 0.5959632868132543,\n",
       "  'support': 15000}}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report\n",
    "# Perform classification with SVM, kernel=linear\n",
    "classifier_linear = svm.SVC(kernel='linear')\n",
    "classifier_linear.fit(X_train_word_average, train['label'])\n",
    "prediction_linear = classifier_linear.predict(X_test_word_average)\n",
    "# results\n",
    "classification_report(test.label, prediction_linear, output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
