{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Revathi\n",
      "[nltk_data]     Bala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data = pd.read_csv('labeled_data.csv')\n",
    "#unlabeled_data = pd.read_csv('unlabeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lab_data = pd.read_csv('./Training Dataset-20191010/labeled_data.csv')\n",
    "#unlabeled_data = pd.read_csv('./Training Dataset-20191010/unlabeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The new rule is - \\r\\nif you are waiting for a table (which you almost always are) , you cant wait inside. \\r\\nThey just posted a sign upfront that it causes some concerns for the seated patrons. \\r\\nHow awful is that? \\r\\n\\r\\nI like that they included the apology along with \"especially now in the cold\". \\r\\n\\r\\np.s you can try calling in ahead to reserve a table but thats ONLY if the waiting list is short. Otherwise, you have to show up to reserve. Boourns. \\r\\n\\r\\n\\r\\nThis place could do no wrong in my eyes. Rattle away you equally-clever/witty-name-for a hot beverage.\\r\\n\\r\\nMust mention - I am obsessed with Mad Gab. \\r\\n\\r\\nxoxo\\r\\nM.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_data['text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Normalisation, Tokenization and Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Revathi\n",
      "[nltk_data]     Bala\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(token_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_token = []\n",
    "    for each in token_list :\n",
    "       # print(each ,\":\", lemmatizer.lemmatize(each)) \n",
    "        lem_token.append(lemmatizer.lemmatize(each))\n",
    "    return lem_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def charac(lem_list):\n",
    " #   list1 =[]\n",
    "  #  for each in lem_list:\n",
    "   #     each = token_nonalpha.tokenize(each)\n",
    "    #    list1.append(each)\n",
    "    #return list1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def nonalpha(lem_list):\n",
    "#    list_1 = []\n",
    "#    for each in lem_list:\n",
    "#        each = re.sub(r\"\\w+(?:[']\\w+)?\",each)\n",
    "#        #each = re.sub(r'\\W+', '', each)\n",
    "#        list_1.append(each)\n",
    "#    return list_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace(list1):\n",
    "    list2 = []\n",
    "    for line in list1 :\n",
    "        line = line.replace('\\r', '')\n",
    "        line = line.replace('\\n', '')\n",
    "        list2.append(line)\n",
    "    return list2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(raw_data):\n",
    "    raw_data1 = raw_data.lower()\n",
    "    tokenised = nltk.tokenize.word_tokenize(raw_data1) \n",
    "    replace_token = replace(tokenised)\n",
    "    lem_token = lemmatization(replace_token)\n",
    "   # neg_token = replaceNegations(lem_token)\n",
    "    #char_token = charac(lem_token)\n",
    "    #stopwords_tokens = [w for w in lem_token if not w in stopwords]\n",
    "   # processed_data = ' '.join(stopwords_tokens) \n",
    "   # nonalpha_token = nonalpha(lem_token)\n",
    "    processed_data = ' '.join(lem_token)\n",
    "    return(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lab_data['text'] = lab_data.apply(lambda row: token(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data['text'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = lab_data['text'].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "\n",
    "vect = TfidfVectorizer(analyzer='word', input='content', ngram_range=(1,2))\n",
    "    \n",
    "train_review = vect.fit_transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# vocab = vect.get_feature_names()\n",
    "\n",
    "# dist = np.sum(train_review.toarray(), axis=0)\n",
    "\n",
    "# for tag, count in zip(vocab, dist):\n",
    "#     print(count, tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_review, lab_data['label'],test_size=0.10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression(random_state=1, solver='liblinear', multi_class='ovr').fit(X_train, y_train)\n",
    "pred=model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#text_classifier = RandomForestClassifier(n_estimators=2000, random_state=0)\n",
    "#text_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions = text_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "#print(confusion_matrix(y_test,predictions))\n",
    "#print(classification_report(y_test,predictions))\n",
    "#print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
