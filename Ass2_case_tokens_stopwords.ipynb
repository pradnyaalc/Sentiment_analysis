{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "import nltk\n",
    "from nltk.tokenize import MWETokenizer\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import gensim\n",
    "import logging\n",
    "from gensim.models.doc2vec import LabeledSentence\n",
    "from gensim.models import word2vec\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data = pd.read_csv('./Training Dataset-20191010/labeled_data.csv')\n",
    "# unlabeled_data = pd.read_csv('./Training Dataset-20191010/unlabeled_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lab_data.head()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_characters(raw_text):\n",
    "    processed_text = re.sub('\\\\n','', raw_text)\n",
    "    processed_text = re.sub('\\\\r','', processed_text)\n",
    "    processed_text = re.sub(\"\\\\'\", \"\\'\",processed_text)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data['text'] = lab_data.apply(lambda row: remove_extra_characters(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case Normalisation, Tokenization and Stop words removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords = f.read().splitlines()\n",
    "stopwords = set(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(token_list):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lem_token = []\n",
    "    for each in token_list :\n",
    "#         print(each ,\":\", lemmatizer.lemmatize(each)) \n",
    "        lem_token.append(lemmatizer.lemmatize(each))\n",
    "    return lem_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(\"\\w+(?:[']\\w+)?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(raw_data):\n",
    "    raw_data1 = raw_data.lower()\n",
    "    tokenised = tokenizer.tokenize(raw_data1)\n",
    "#     tokenised = nltk.tokenize.word_tokenize(raw_data1)\n",
    "    lem_token = lemmatization(tokenised)\n",
    "#     stopwords_tokens = [w for w in tokenised if not w in stopwords]\n",
    "    processed_data = ' '.join(lem_token)\n",
    "        \n",
    "    return(processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lab_data['text'] = lab_data.apply(lambda row: token(row['text'].strip()), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab_data['text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF + Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer \n",
    "\n",
    "\n",
    "vectorizer = TfidfVectorizer(lowercase = True,analyzer = 'word',ngram_range = (1,2))\n",
    "    \n",
    "train_review = vectorizer.fit_transform(lab_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_review, lab_data['label'],test_size=0.20, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_cross_val(model):\n",
    "    # perfroming 10 fold cross validation\n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    params = {}\n",
    "    nb = model\n",
    "    gs = GridSearchCV(nb, cv=skf, param_grid=params, return_train_score=False)\n",
    "    return gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "gs = instantiate_cross_val(model)\n",
    "\n",
    "clf=gs.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('parameters:', clf.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create regularization penalty space\n",
    "penalty = ['l1', 'l2']\n",
    "\n",
    "# Create regularization hyperparameter space\n",
    "C = np.logspace(0, 4, 10)\n",
    "\n",
    "# multi_class = ['multinomial','ovr']\n",
    "\n",
    "\n",
    "# Create hyperparameter options\n",
    "hyperparameters = dict(C=C, penalty=penalty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(model, hyperparameters, cv=10, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View best hyperparameters\n",
    "print('Best Penalty:', best_model.best_estimator_.get_params()['penalty'])\n",
    "print('Best C:', best_model.best_estimator_.get_params()['C'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best C:', best_model.best_estimator_.get_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_best_pred = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_best_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(random_state=1, C=1.5, solver='sag', multi_class = 'multinomial')\n",
    "log_model.fit(X_train, y_train)\n",
    "y_pred = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6111\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    import itertools\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plot_confusion_matrix(cm, classes=['1', '2', '3', '4', '5'],\n",
    "                      title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probab = clf.predict_proba(test_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_probab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_test = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(pred_probab)):\n",
    "    p_test.append(max(pred_probab[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.DataFrame({'text':X_test, 'label':y_test, 'p_test':p_test, 'y_pred':y_pred})\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data[(train_data['p_test'] > 0.9) & (train_data['label']==train_data['y_pred'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec + Logistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"flirted with giving this two star but that's a pretty damning rating for what might have just been an off night new to the east side and so we don't know many of these hidden gem but me and the fiance met her friend for drink here and ended up getting some thing to nibble first off service wa pretty slow which wa unusual because the restaurant is pretty small and galley style you would think it would be easy for server to routinely hit up table a you pas by the fiance ordered the quinoa salad and said it wa pretty good but dry i wasn't too hungry and so i simply ordered the bruchetta 3 way which came with burnt crostinis and i ordered a side of fry which were either hard or chewy the friend ordered the macaroni cheese and added chicken and bacon her usual order and liked it can't remember the last time i thought to myself huh they failed at fry so like i said two star but the decor wa good it wa a good place to have a conversation and i might be back to try more expensive fare but ah the fry thing yeeesh i dunno man\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lab_data['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = []\n",
    "for review in lab_data['text']:\n",
    "    sentences.append(review.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['flirted',\n",
       " 'with',\n",
       " 'giving',\n",
       " 'this',\n",
       " 'two',\n",
       " 'star',\n",
       " 'but',\n",
       " \"that's\",\n",
       " 'a',\n",
       " 'pretty',\n",
       " 'damning',\n",
       " 'rating',\n",
       " 'for',\n",
       " 'what',\n",
       " 'might',\n",
       " 'have',\n",
       " 'just',\n",
       " 'been',\n",
       " 'an',\n",
       " 'off',\n",
       " 'night',\n",
       " 'new',\n",
       " 'to',\n",
       " 'the',\n",
       " 'east',\n",
       " 'side',\n",
       " 'and',\n",
       " 'so',\n",
       " 'we',\n",
       " \"don't\",\n",
       " 'know',\n",
       " 'many',\n",
       " 'of',\n",
       " 'these',\n",
       " 'hidden',\n",
       " 'gem',\n",
       " 'but',\n",
       " 'me',\n",
       " 'and',\n",
       " 'the',\n",
       " 'fiance',\n",
       " 'met',\n",
       " 'her',\n",
       " 'friend',\n",
       " 'for',\n",
       " 'drink',\n",
       " 'here',\n",
       " 'and',\n",
       " 'ended',\n",
       " 'up',\n",
       " 'getting',\n",
       " 'some',\n",
       " 'thing',\n",
       " 'to',\n",
       " 'nibble',\n",
       " 'first',\n",
       " 'off',\n",
       " 'service',\n",
       " 'wa',\n",
       " 'pretty',\n",
       " 'slow',\n",
       " 'which',\n",
       " 'wa',\n",
       " 'unusual',\n",
       " 'because',\n",
       " 'the',\n",
       " 'restaurant',\n",
       " 'is',\n",
       " 'pretty',\n",
       " 'small',\n",
       " 'and',\n",
       " 'galley',\n",
       " 'style',\n",
       " 'you',\n",
       " 'would',\n",
       " 'think',\n",
       " 'it',\n",
       " 'would',\n",
       " 'be',\n",
       " 'easy',\n",
       " 'for',\n",
       " 'server',\n",
       " 'to',\n",
       " 'routinely',\n",
       " 'hit',\n",
       " 'up',\n",
       " 'table',\n",
       " 'a',\n",
       " 'you',\n",
       " 'pas',\n",
       " 'by',\n",
       " 'the',\n",
       " 'fiance',\n",
       " 'ordered',\n",
       " 'the',\n",
       " 'quinoa',\n",
       " 'salad',\n",
       " 'and',\n",
       " 'said',\n",
       " 'it',\n",
       " 'wa',\n",
       " 'pretty',\n",
       " 'good',\n",
       " 'but',\n",
       " 'dry',\n",
       " 'i',\n",
       " \"wasn't\",\n",
       " 'too',\n",
       " 'hungry',\n",
       " 'and',\n",
       " 'so',\n",
       " 'i',\n",
       " 'simply',\n",
       " 'ordered',\n",
       " 'the',\n",
       " 'bruchetta',\n",
       " '3',\n",
       " 'way',\n",
       " 'which',\n",
       " 'came',\n",
       " 'with',\n",
       " 'burnt',\n",
       " 'crostinis',\n",
       " 'and',\n",
       " 'i',\n",
       " 'ordered',\n",
       " 'a',\n",
       " 'side',\n",
       " 'of',\n",
       " 'fry',\n",
       " 'which',\n",
       " 'were',\n",
       " 'either',\n",
       " 'hard',\n",
       " 'or',\n",
       " 'chewy',\n",
       " 'the',\n",
       " 'friend',\n",
       " 'ordered',\n",
       " 'the',\n",
       " 'macaroni',\n",
       " 'cheese',\n",
       " 'and',\n",
       " 'added',\n",
       " 'chicken',\n",
       " 'and',\n",
       " 'bacon',\n",
       " 'her',\n",
       " 'usual',\n",
       " 'order',\n",
       " 'and',\n",
       " 'liked',\n",
       " 'it',\n",
       " \"can't\",\n",
       " 'remember',\n",
       " 'the',\n",
       " 'last',\n",
       " 'time',\n",
       " 'i',\n",
       " 'thought',\n",
       " 'to',\n",
       " 'myself',\n",
       " 'huh',\n",
       " 'they',\n",
       " 'failed',\n",
       " 'at',\n",
       " 'fry',\n",
       " 'so',\n",
       " 'like',\n",
       " 'i',\n",
       " 'said',\n",
       " 'two',\n",
       " 'star',\n",
       " 'but',\n",
       " 'the',\n",
       " 'decor',\n",
       " 'wa',\n",
       " 'good',\n",
       " 'it',\n",
       " 'wa',\n",
       " 'a',\n",
       " 'good',\n",
       " 'place',\n",
       " 'to',\n",
       " 'have',\n",
       " 'a',\n",
       " 'conversation',\n",
       " 'and',\n",
       " 'i',\n",
       " 'might',\n",
       " 'be',\n",
       " 'back',\n",
       " 'to',\n",
       " 'try',\n",
       " 'more',\n",
       " 'expensive',\n",
       " 'fare',\n",
       " 'but',\n",
       " 'ah',\n",
       " 'the',\n",
       " 'fry',\n",
       " 'thing',\n",
       " 'yeeesh',\n",
       " 'i',\n",
       " 'dunno',\n",
       " 'man']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model...\n"
     ]
    }
   ],
   "source": [
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 10   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "# context = 10          # Context window size                                                                                    \n",
    "\n",
    "# Initialize and train the model (this will take some time)\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model...\")\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count)\n",
    "\n",
    "# If you don't plan to train the model any further, calling \n",
    "# init_sims will make the model much more memory-efficient.\n",
    "# model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `doesnt_match` (Method will be removed in 4.0.0, use self.wv.doesnt_match() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.vectors instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(12109, 200)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.syn0.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "list(islice(model.wv.vocab, 11030, 13050))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(lab_data, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def w2v_tokenize_text(text):\n",
    "    tokens = text.split(' ')\n",
    "    return tokens\n",
    "\n",
    "test_tokenized = test.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values\n",
    "train_tokenized = train.apply(lambda r: w2v_tokenize_text(r['text']), axis=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['i', 'found', 'this', 'place', 'to', 'be', 'overpriced', 'especially', 'the', 'food', 'the', 'appetizer', 'we', 'got', 'were', 'not', 'worth', 'the', 'money', 'especially', 'when', 'it', 'came', 'to', 'the', 'portion', 'if', 'i', 'do', 'come', 'back', \"i'll\", 'make', 'sure', 'to', 'eat', 'before', 'i', 'go', 'but', 'i', 'think', 'this', 'would', 'be', 'a', 'good', 'place', 'to', 'go', 'when', 'the', 'weather', 'is', 'nice', 'since', 'the', 'back', 'patio', 'look', 'relaxing', 'if', 'it', \"weren't\", 'for', 'that', \"i'm\", 'not', 'sure', 'i', 'would', 'return']),\n",
       "       list(['delicious', 'food', 'good', 'coffee', 'very', 'friendly', 'food', 'portion', 'were', 'average', 'for', 'la', 'vega', 'my', 'youngest', 'had', 'pancake', 'with', 'egg', 'and', 'bacon', 'pancake', 'were', 'large', 'fluffy', 'and', 'moist', 'hubby', 'had', 'corn', 'beef', 'hash', 'with', 'over', 'easy', 'egg', 'and', 'it', 'wa', 'hash', 'a', 'i', 'have', 'never', 'seen', 'it', 'like', 'cut', 'up', 'slice', 'of', 'corn', 'beef', 'tasted', 'pretty', 'good', 'and', 'not', 'greasy', 'at', 'all', 'older', 'daughter', 'had', 'egg', 'burrito', 'with', 'chorizo', 'she', 'loved', 'every', 'bit', 'and', 'opted', 'for', 'fresh', 'fruit', 'in', 'place', 'of', 'potato', 'piece', 'of', 'watermelon', 'cantaloupe', 'and', 'grapefruit', 'i', 'had', 'scrambled', 'egg', 'with', 'green', 'onion', 'and', 'cream', 'cheese', 'side', 'of', 'hash', 'brown', 'and', 'in', 'place', 'of', 'toast', 'coffee', 'cake', 'coffee', 'cake', 'wa', 'amazing', 'super', 'moist', 'and', 'perfect', 'amount', 'of', 'cinnamon', 'delicious', 'my', 'egg', 'with', 'cream', 'cheese', 'and', 'green', 'onion', 'were', 'flavorful', 'and', 'moist', 'coffee', 'hot', 'and', 'strong', 'service', 'fast', 'friendly', 'and', 'server', 'stopped', 'by', 'many', 'time', 'to', 'refill', 'beverage', 'bathroom', 'cramped', 'but', 'very', 'clean', 'only', '4', 'star', 'because', 'i', 'feel', 'the', 'price', 'wa', 'a', 'bit', 'high', 'over', '50', '00', 'for', 'our', 'meal', 'i', \"don't\", 'know', 'if', 'that', 'is', 'average', 'for', 'off', 'the', 'strip', 'but', 'for', 'u', 'so', 'cal', 'family', \"it's\", 'a', 'bit', 'high', 'with', 'all', 'that', 'said', 'will', 'go', 'back', 'again']),\n",
       "       list(['i', 'got', 'an', 'odd', 'feeling', 'that', 'i', 'wa', 'in', 'a', 'flea', 'market', 'of', 'crazy', 'expensive', 'watch', 'must', 'have', 'been', 'the', 'salesperson', 'either', 'pushy', 'or', 'distant', \"i'll\", 'stick', 'with', 'our', 'baby', 'tourneau', 'in', 'scottsdale']),\n",
       "       ...,\n",
       "       list(['great', 'staff', 'great', 'vet', 'had', 'a', 'great', 'experience', 'with', 'my', 'new', 'puppy', 'they', 'also', 'have', 'really', 'good', 'price', 'with', 'different', 'package']),\n",
       "       list(['have', 'loved', 'this', 'restaurant', 'since', 'the', 'first', 'time', 'i', 'ever', 'ate', 'here', 'i', 'own', 'a', 'salon', 'in', 'the', 'area', 'and', 'can', 'say', 'that', 'il', 'bosco', 'is', 'responsible', 'for', 'feeding', 'u', 'multiple', 'day', 'out', 'of', 'the', 'week', 'love', 'the', 'changing', 'special', 'and', 'charm', 'of', 'the', 'quaint', 'atmosphere', 'staff', 'is', 'friendly', 'and', 'accommodating', 'but', 'not', 'overly', 'intrusive', 'love', 'the', 'caprese', 'panini', 'and', 'the', 'biaggo', 'pizza', 'all', 'the', 'food', 'is', 'extremely', 'fresh', 'and', 'always', 'tasty']),\n",
       "       list(['my', 'nail', 'tech', 'candy', 'deserves', '5', 'star', 'she', 'is', 'friendly', 'and', 'doe', 'a', 'great', 'job', 'the', 'salon', 'is', 'also', 'pretty', 'and', 'nicely', 'decorated', 'however', 'they', 'use', 'horrible', 'polish', 'the', 'selection', 'of', 'color', 'is', 'minimal', 'i', 'paid', 'extra', 'for', 'gel', 'and', 'it', 'peeled', 'the', 'next', 'day', 'my', 'toe', 'had', 'regular', 'polish', 'and', 'they', \"didn't\", 'last', '5', 'day', 'i', 'had', 'a', 'package', 'of', 'groupons', 'and', 'each', 'time', 'my', 'polish', 'lasted', '1', '5', 'day', 'i', 'recently', 'thought', \"i'd\", 'give', 'it', 'another', 'try', 'because', 'i', 'like', 'the', 'tech', 'so', 'much', 'but', 'they', \"won't\", 'respond', 'to', 'my', 'message', 'i', 'asked', 'if', 'she', 'worked', 'there', 'and', 'if', \"they've\", 'changed', 'the', 'polish', 'they', 'use', 'and', 'they', \"won't\", 'text', 'back', 'pretty', 'disappointing'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test_tokenized = test['text'].values\n",
    "train_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_averaging(wv, words):\n",
    "    all_words, mean = set(), []\n",
    "    \n",
    "    for word in words:\n",
    "        if isinstance(word, np.ndarray):\n",
    "            mean.append(word)\n",
    "        elif word in wv.vocab:\n",
    "            mean.append(wv.syn0norm[wv.vocab[word].index])\n",
    "            all_words.add(wv.vocab[word].index)\n",
    "\n",
    "    if not mean:\n",
    "        logging.warning(\"cannot compute similarity with no input %s\", words)\n",
    "        # FIXME: remove these examples in pre-processing\n",
    "        return np.zeros(wv.vector_size,)\n",
    "\n",
    "    mean = gensim.matutils.unitvec(np.array(mean).mean(axis=0)).astype(np.float32)\n",
    "    return mean\n",
    "\n",
    "def  word_averaging_list(wv, text_list):\n",
    "    return np.vstack([word_averaging(wv, review) for review in text_list ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.vectors_norm instead).\n",
      "  \n",
      "WARNING:root:cannot compute similarity with no input ['']\n",
      "WARNING:root:cannot compute similarity with no input ['']\n"
     ]
    }
   ],
   "source": [
    "X_train_word_average = word_averaging_list(model.wv,train_tokenized)\n",
    "X_test_word_average = word_averaging_list(model.wv,test_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy 0.5736\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression(C=1.5, solver='sag', multi_class = 'multinomial')\n",
    "logreg.fit(X_train_word_average, train['label'])\n",
    "y_pred = logreg.predict(X_test_word_average)\n",
    "print('accuracy %s' % metrics.accuracy_score(y_pred, test.label))\n",
    "# print(classification_report(test.label, y_pred,target_names=my_tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
